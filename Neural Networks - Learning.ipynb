{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost Function and Backpropagation\n",
    "\n",
    "## Cost Function\n",
    "\n",
    "Some variables that will be used\n",
    "\n",
    "* L = total number of layers in the network\n",
    "* sl = number of units (not counting bias unit) in layer l\n",
    "* K = number of output units / classes\n",
    "\n",
    "The following cost function is a generalization of the cost function of logistic regression\n",
    "\n",
    "![Back Propagation Cost Function](Resources/bpcf.png \"Back Propagation Cost Function\")\n",
    "\n",
    "* The first term is the sum of costs (w/o regularization) over all output units\n",
    "* The second (regularization) term just sums up all the theta in the matrices (squared)\n",
    "\n",
    "## Back Propagation Algorithm\n",
    "\n",
    "For each training element:\n",
    "\n",
    "1. Forward Propagation with an initial theta matrices -> We have an hypothesis output\n",
    "2. Calculate the error rate for the output layer (yi - hypo output)\n",
    "3. Backpropagate by calculating delta of the previous layers (nodes) by weighing it with the specific theta\n",
    "\n",
    "-> The result are the partial derivatives of the cost function (derived by the thetas)\n",
    "\n",
    "![Back Propagation](Resources/bpalgo.png \"Back Propagation\")\n",
    "\n",
    "Intuitively: Forward propagation is from left to right, Backward propagation is from right to left\n",
    "\n",
    "![Back Intuition](Resources/bpintuition.png \"Back Propagation Intution\")\n",
    "\n",
    "### Learning Algorithm\n",
    "\n",
    "The back propagation computes the partial derivatives of the cost function (for the thetas). They return the gradients as well as the costs. The outputs of this function are used for fminunc. The gradients as well as the thetas matrices(thetas for fminunc, not for back prop.) have to be unrolled into vectors. \n",
    "\n",
    "#### Unroll a matrice into a vector\n",
    "thetaVector = [ Theta1(:); Theta2(:); Theta3(:); ]\n",
    "\n",
    "#### Reshape an unrolled vector into matrices\n",
    "Theta1 = reshape(thetaVector(1:110),10,11)\n",
    "\n",
    "![Learning Algorithm](Resources/learningalgo.png \"Learning Algorithm\")\n",
    "\n",
    "### Gradient checking\n",
    "\n",
    "Gradient checking is used to debug your back propagation algorithm. With gradient checking, an approximation of the gradients (for each theta) is calculated using the following formula:\n",
    "\n",
    "![Gradient checking](Resources/gradientcheck.png \"Gradient checking\")\n",
    "\n",
    "Then we test whether our back propagation algorithm returns gradients that are approx. the ones computed numerically (formula above). If this is the case, our back propagation algorithm works fine.\n",
    "\n",
    "<font color='red'> Note: When NN training is conducted, turn gradient checking off since it is very slow. That's also why we don't directly use the approximation, which would be easier to implement, but much slower. </font>\n",
    "\n",
    "### Random initialization\n",
    "\n",
    "It can be shown that if the initial values of all thetas are equal (e.g. 0), then back propagation will not work correctly, since each unit in a layer will have the same value. Therefore, we initialize each theta with a random value between (-epsilon, epsilon). \n",
    "\n",
    "Theta1 = rand(10,11) * (2 * INIT_EPSILON) - INIT_EPSILON;\n",
    "\n",
    "As an example, this creates a 10x11 matrix (for example for the first layer of the neural network) of thetas, where each value is a random value between epsilon and - epsilon.\n",
    "\n",
    "### Putting it all together\n",
    "\n",
    "#### Neural Network design\n",
    "\n",
    "* Input layer is defined by your data (e.g. 20x20 image are 400 input neurons, 1 for each pixel)\n",
    "* Output layer is defined by how many classes you want to classify (1 for two classes, n for n classes where n >= 3)\n",
    "* Hidden layer:\n",
    "    * Default: 1 Hidden layer, if you have more, number of units per layer identical\n",
    "    * Usually the more units you have the better, trade-off with computational time\n",
    "\n",
    "#### Training a Neural Network\n",
    "\n",
    "1. Randomly initialize Theta\n",
    "2. Implement forward propagation to compute h(x)\n",
    "3. Implement cost function\n",
    "4. Implement backward propagation to compute the gradient (all partial derivatives)\n",
    "5. Gradient checking, then disable it (since its slow)\n",
    "6. Use the output (costs and gradient) for gradient descent or other more advanced optimization solvers\n",
    "\n",
    "<b>Keep in mind that forward and backward propagation is looped over every training example.</b>\n",
    "\n",
    "<font color='red'>The function is not convex, meaning existence of local optima. Turns out not to be a problem.</font>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
