{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (ML)\n",
    "\n",
    "The goal of this notebook is to document the learning process of the ML course on coursera.org\n",
    "\n",
    "## ML fundamentals\n",
    "\n",
    "### What is ML\n",
    "\n",
    "A program (or machine) is said to learn if its performance P on task T is increasing with experience E.\n",
    "\n",
    "Example: An algorithm that predicts the median housing prices in a given area.\n",
    "\n",
    "- T is the prediction of the median housing price in an area given its other features\n",
    "- E is the median housing price data the algorithm gets\n",
    "- P is the algorithm's prediction accuracy\n",
    "\n",
    "### Types of ML\n",
    "\n",
    "ML approaches can be divided by the amount of supervision they get (supervised, unsupervised, semisupervised, reinforcement learning), whether the algorithm learns incrementally or from scratch (batch vs. online) and whether it learns by heart (instance-based) or builds a predictive model (model based). Furthermore, we will mainly focus on supervised vs. unsupervised learning (the other distinctions can also be used to describe the algorithms).\n",
    "\n",
    "#### Supervised learning\n",
    "\n",
    "In supervised learning the algorithm receives training data that is labeled, meaning a target feature is given. TODO\n",
    "\n",
    "#### Unsupervised learning\n",
    "\n",
    "TODO\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Plot the cost function and derive the minimum over all Theta-sets (sets of different parameter combinations)\n",
    "\n",
    "## Gradient descent\n",
    "\n",
    "Algorithm that minimizes any cost functions (or other functions)\n",
    "\n",
    "![title](http://localhost:8888/tree/CourseraML/gradientdescent.jpg)\n",
    "\n",
    "As you can see the algorithm converges step-wise to a local minimum (how to solve for global will come later, as the MSE cost function for linear regression is a convex function (bow shaped), meaning it has no local optima, only a global one. So the picture above does not represent such a function. It only shows, that the GD on this function can actually result in two different optima, depending on its starting point.\n",
    "\n",
    "![title](http://localhost:8888/tree/CourseraML/algorithm_gs.jpg)\n",
    "\n",
    "A simultaneous update of all the parameters is key to a correct implementation of GD. \n",
    "Derivative: The derivative is calculated because it shows the slope (tangent) at any given point, so that we can determine where the \"steepest\" descent from a given point. Calculate this step-wise for every function et voila. We always have to go \"downwards\" meaning if the slope is positive, we have to subtract it, otherwise perform an addition.\n",
    "Alpha: Alpha in the equation is called the learning parameter. This parameter defines the step sizes. But beware (!): If it is too small, GD might take a long time, and if it too large, it might overshoot the (local) optima.\n",
    "\n",
    "![title](http://localhost:8888/tree/CourseraML/gd_step_importance.png)\n",
    "\n",
    "Even if Alpha is fixed, GD will converge because the slope will become smaller and smaller and a factor 0 will occur (for deravative) if we are at an optimum. This means that the learning rate does not have to be adjusted.\n",
    "\n",
    "Batch vs \"online\" gradient descent\n",
    "-> Batch uses all the training data\n",
    "-> Online only partial\n",
    "\n",
    "## Linear Algebra in ML\n",
    "\n",
    "Efficiently compute values using matrix and vector operations (multiplication, addition, subtraction)\n",
    "\n",
    "![title](http://localhost:8888/tree/CourseraML/mxm_mult_hypothesis.jpg)\n",
    "\n",
    "As you can see, for each hypothesis (matrix to the right) and the given data (housing size), the specific housing price is calculated. To note here is how fast different hypothesis and their respective values can be computed, so that things like GD can be executed really fast.\n",
    "\n",
    "### Properties of Matrix Multiplication\n",
    "\n",
    "1. Note Matrix Multiplication is not commutative (except identity matrix). Values and dimensions don't even match! \n",
    "2. MM is associative (Ax(BxC) = (AxB)xC)\n",
    "\n",
    "Identity matrix: Number 1 is identity (1 on the diagonal, rest 0)\n",
    "Denoted as I: AxI = IxA = A (still has to fulfill mxn nxk criteria etc.)\n",
    "\n",
    "Matrix Inverse: A x A^-1 = I\n",
    "\n",
    "Matrix Transverse: Transponiert (90 grad clockwise rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
