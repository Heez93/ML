{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating a Learning Algorithm\n",
    "\n",
    "### General\n",
    "\n",
    "#### What if my model does not perform well?\n",
    "There are certain avenues to pursue\n",
    "* Increase / Decrease Lambda\n",
    "* Tweak features (more, less, poly)\n",
    "* Get more data etc.\n",
    "\n",
    "#### How do people decide which one to take?\n",
    "Usually by gut-feeling. But there are some ML diagnostics that can be implemented to show which road should or should not be pursued.\n",
    "\n",
    "#### Evaluate a Learning Algorithm\n",
    "* Split training data into test and train set\n",
    "* Train on train set, test on test set\n",
    "* compute the cost (use specific cost function) for the test set\n",
    "* For logistic regression use the cost function, for classification use the fraction of misclassified test examples\n",
    "\n",
    "#### Model selection - Train, (Cross) validation & Test set\n",
    "\n",
    "In model selection you want to test out different hypothesis with different degrees of freedom (x^d) and see which hypothesis performs best. One could minimize the cost function using the training set for different hypothesis, compute the costs for each of them and choose the one with the lowest cost. The problem here is that this model would be <b> fit to the test test </b>. \n",
    "Therefore, a <b> validation set</b> should be used to determine the best model (min Cost) and then test how well it performs on a seperate test set.\n",
    "\n",
    "* 60% training set -> train different models\n",
    "* 20% validation set -> try out different degrees of freedom and choose the one with the lowest cost\n",
    "* 20% test set -> compute the generalization error\n",
    "\n",
    "### Bias (underfit) vs. variance (overfit)\n",
    "\n",
    "We want to choose d (degrees of freedom) that will generalize well to new examples. To achieve this, we need to have a balance between high bias and high variance:\n",
    "\n",
    "![High Bias vs. High Variance](Resources/biasvsvariance.png \"High Bias vs. High Variance\")\n",
    "\n",
    "* High bias (underfitting): High training and cross validation error\n",
    "* High variance (overfitting): Low training but high cross validation error\n",
    "\n",
    "#### With reguralization\n",
    "\n",
    "![High Bias vs. High Variance](Resources/lambdabiasvariance.png \"High Bias vs. High Variance\")\n",
    "\n",
    "-> Compute the cross validation error without regularization\n",
    "\n",
    "* Training set: The higher the lambda, the bigger the error (underfit) since the parameters converge to 0\n",
    "* Validation set: No regularization results in an overfit, so high error, and hight lambda into underfit, e.g. again high error\n",
    "\n",
    "#### When is more training data (not) helpful?\n",
    "\n",
    "* It is not helpful if my learning algorithm suffers <b> high bias (underfitting) </b>\n",
    "![High Bias vs. High Variance](Resources/learningcurve01.png \"High Bias vs. High Variance\")\n",
    "* It is helpful if my learning algorithm suffers <b> high variance (overfitting) </b>\n",
    "![High Bias vs. High Variance](Resources/learningcurve02.png \"High Bias vs. High Variance\")\n",
    "\n",
    "\n",
    "#### Diagnosing Neural Networks\n",
    "\n",
    "* NN with few parameters: Computationally cheaper, but prone to underfitting\n",
    "* NN with many parameters: Computationally expensive and prone to overfitting (use reguralization)\n",
    "\n",
    "#### Fixes to problems\n",
    "\n",
    "* Getting more training examples: Fixes high variance \n",
    "* Trying smaller sets of features: Fixes high variance\n",
    "* Adding features: Fixes high bias\n",
    "* Adding polynomial features: Fixes high bias\n",
    "* Decreasing lambda: Fixes high bias\n",
    "* Increasing lambda: Fixes high variance\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
