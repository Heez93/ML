{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Linear Regression\n",
    "\n",
    "## Notation\n",
    "\n",
    "![title](http://localhost:8888/tree/CourseraML/multiple_features.jpg)\n",
    "![title](http://localhost:8888/tree/CourseraML/hypothesis_notation.jpg)\n",
    "\n",
    "## Making GD faster - Feature Scaling & Mean normalization\n",
    "\n",
    "Make sure that features are on a similar scale (!)\n",
    "Regarding contour graph: If two variables are not scaled properly, the contour graph will be a very slim elipse (2 dimensional case)\n",
    "\n",
    "![title](http://localhost:8888/tree/CourseraML/contour_elipse.jpg)\n",
    "\n",
    "Gradient descent would take longer (harder for it) to go to optimum\n",
    "In general, the optimal case would be to have each variable scaled on -1 to +1, but each variable can be scaled differently and it can also be further away. But the closer, the faster GD is.\n",
    "\n",
    "Feature Scaling: Input value / range (e.g. max - min) -> results in a range of 1\n",
    "\n",
    "Mean normalization: Input value - average\n",
    "\n",
    "To implement both:\n",
    "\n",
    "![title](http://localhost:8888/tree/CourseraML/featurescaling.jpg)\n",
    "\n",
    "xi = Input value\n",
    "ui = Average of the attribute\n",
    "si = standard deviation\n",
    "\n",
    "You can also divide by range (gives different results). \n",
    " \n",
    "This is one method (of probably many). The end goal is to have a small-ranged scale for each feature -> And finally, this enhances the speed of the gradient descent algorithm.\n",
    "\n",
    "## How to set the learning parameter\n",
    "\n",
    "You can debug it while letting the gradient descent be plotted\n",
    "![title](http://localhost:8888/tree/CourseraML/debugging_gd.jpg)\n",
    "Cost function should decrease after each iteration (check it!)\n",
    "You can declare an automatic convergence test (Declare convergence if the decreasing step is small enough)\n",
    "\n",
    "In case of overshooting, choose a smaller alpha.\n",
    "For up down, up down also choose smaller alpha.\n",
    "\n",
    "For sufficiently small alpha, the cost function decreases every iteration (but risk of being too slow)\n",
    "Try setting it in three step approaches (0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1.0)\n",
    "\n",
    "## Polynomial regression\n",
    "\n",
    "Is about combining different features\n",
    "We can change the behavior of the curve of hypothesis by making it a quadratic, cubic or square root function (or further forms)\n",
    "\n",
    "Feature scaling becomes increasingly important\n",
    "Polynomial functions are used\n",
    "We have algorithms that help us decide between linear, quad, cubic function\n",
    "\n",
    "## Normal equation method\n",
    "\n",
    "Solve to minimize J analytically (not iterative like Gradient Descent)\n",
    "![title](http://localhost:8888/tree/CourseraML/solveanalytically.jpg)\n",
    "![title](http://localhost:8888/tree/CourseraML/analytical.jpg)\n",
    "The equation in red solves for the minimum.\n",
    "\n",
    "With this method, feature scaling is not necessary.\n",
    "\n",
    "#### Weighing of Normal equation method vs. Gradient Descent\n",
    "\n",
    "Gradient Descent:\n",
    "\n",
    "- Needs to set alpha\n",
    "- Needs many iterations\n",
    "- Works well if n is large\n",
    "- Good for many different methods\n",
    "\n",
    "Normal Equation:\n",
    "\n",
    "- No need to set alpha\n",
    "- No need for iterations\n",
    "- Has to compute Inverse(XT*X)\n",
    "- Slow for large n since Inverse comp costs O(n^3)\n",
    "- Does work mainly for linear regression\n",
    "\n",
    "What is small n / large n? Small n below 10000 n, large n above 10000 roughly\n",
    "n is the number of features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
